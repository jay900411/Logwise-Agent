A 終端機 (Ollama):
./ollama serve

B 終端機 (WebUI)：
streamlit run logwise/webui/app.py --server.port 8501

C 終端機 (工作區 / 代理)：

cd /home/emo/work/StableSR
source .venv/bin/activate
python runner.py

python ../logwise/runner.py

###### dowload ollama (linux)

mkdir -p ~/bin
cd ~/bin

wget https://github.com/ollama/ollama/releases/latest/download/ollama-linux-amd64.tgz
tar -xvzf ollama-linux-amd64.tgz

mv ./bin/ollama ~/bin/ollama
chmod +x ~/bin/ollama

#####啟動伺服器 & check

~/bin/ollama serve
~/bin/ollama --version

#####下載模型

~/bin/ollama pull qwen2.5:7b

###### 另開一個 terminal 測試 API
curl http://127.0.0.1:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "Hello! What is 2+2?"
  }'

##### 建立python folder
mkdir -p ~/work/logwise
cd ~/work/logwise

logwise/
├── __init__.py
├── __main__.py
├── cli.py
├── logwise_cli.py
├── error_extractor.py
└── webui/
    ├── __init__.py
    └── app.py       ← Streamlit 入口

##### 使用說明

python3 -m logwise

##### 模擬簡單錯誤

echo "RuntimeError: CUDA out of memory" | python3 -m logwise

##### 模擬找不到命令
python3 -m logwise run "ls /no_such_dir"

###### webui  (Streamlit)
pip install streamlit requests

streamlit run logwise/webui/app.py --server.port 8501
###### 背景執行
nohup streamlit run logwise/webui/app.py --server.port 8501 > webui.log 2>&1 &
disown

ps aux | grep streamlit                        (查看)
pkill -f "streamlit run logwise/webui/app.py"   (關閉)




